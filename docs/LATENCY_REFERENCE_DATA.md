# 延迟参考数据 - LMAX架构师分享的典型延迟数据

本文档整理了LMAX架构师（Martin Thompson等）在多个场合分享的硬件和软件典型延迟数据，这些数据是高性能系统设计的重要参考。

## 时钟周期计算（3GHz CPU）

- **3GHz CPU** = 3,000,000,000 Hz
- **1个时钟周期** = 1/3,000,000,000 秒 = **0.333纳秒**

---

## 表1：硬件典型延迟数据（3GHz CPU）

### LMAX架构师分享的数据

LMAX架构师（Martin Thompson等）在"Mechanical Sympathy"演讲中分享的硬件延迟数据：

| 操作 | 典型延迟 | 3GHz时钟周期 | 说明 |
|------|---------|-------------|------|
| **L1 cache hit** | **0.5ns** | 1-2周期 | 核心私有，基准 |
| **L2 cache hit** | **3-5ns** | 9-15周期 | 片上缓存，比L1慢6-10倍 |
| **L3 cache hit** | **10-20ns** | 30-60周期 | 跨socket核心共享 |
| **DDR4主内存（随机读）** | **60-80ns** | 180-240周期 | 跨缓存到主内存 |
| **StoreLoad barrier** | **15-25ns** | 45-75周期 | x86内存可见性同步 |
| **Lock-free CAS（成功）** | **15-30ns** | 45-90周期 | 无锁编程基础 |
| **Mutex lock/unlock（无竞争）** | **25-50ns** | 75-150周期 | 有竞争时：微秒级 |
| **Thread context switch** | **1-5μs** | 3000-15000周期 | 内核调度 + 缓存失效 |
| **Disruptor（共享内存，无锁）** | **50-200ns** | 150-600周期 | 零拷贝 + 无锁，线程间直接通信 |
| **Aeron IPC（共享内存）** | **300ns-1μs** | 900-3000周期 | 低开销协议 + 无锁 |
| **localhost UDP（CPU-bound + zero-copy）** | **500ns-2μs** | 1500-6000周期 | p50延迟，p99: < 5μs |
| **localhost TCP（NODELAY）** | **5-10μs** | 15000-30000周期 | p50延迟，p99: 10-20μs |

**数据来源**：LMAX架构师（Martin Thompson等）在"Mechanical Sympathy"演讲中分享  
**关键洞察**：Disruptor/Aeron设计哲学是将软件开销最小化到"硬件基准 + 最小协议/调度开销"，通过缓存行填充、无锁CAS、零拷贝技术将操作保持在最小周期数内。

### 补充数据（硬件厂商和云服务商）

以下数据来自硬件厂商规格、云服务商文档和实际测试：

| 操作 | 典型延迟 | 3GHz时钟周期 | 说明 |
|------|---------|-------------|------|
| **HDD随机读取** | **2-12ms** | 6M-36M周期 | 7200 RPM: ~8-12ms, 15000 RPM: ~2ms |
| **SATA SSD随机读取** | **50-100μs** | 150K-300K周期 | 顺序读取: ~500μs |
| **NVMe SSD随机读取** | **20-100μs** | 60K-300K周期 | 顺序读取: ~100μs |
| **Cilium eBPF转发** | **5-50μs** | 15K-150K周期 | 跨AZ RTT: ~0.55ms |
| **云商同机房网络** | **10-100μs** | 30K-300K周期 | 同数据中心，跨AZ: 0.1-1ms |
| **RDMA（同机房）** | **1-10μs** | 3K-30K周期 | 绕过内核，直接内存访问 |

**数据来源**：硬件厂商规格、云服务商文档、实际测试数据

---

## 表2：Disruptor软件延迟数据（Three-Stage Pipeline测试）

LMAX Disruptor在三阶段流水线测试中的延迟表现：

| 指标 | 延迟 | 说明 |
|------|------|------|
| **Min Latency** | **29ns** | 最小延迟（单跳） |
| **Mean Latency** | **52ns** | 平均延迟（单跳） |
| **99% Latency** | **128ns** | 99%观测值小于此值 |
| **99.99% Latency** | **8,192ns** | 99.99%观测值小于此值 |
| **Max Latency** | **175,567ns** | 最大延迟（极端情况） |

**测试环境**：2.2GHz Core i7-2720QM, Java 1.6.0_25, Ubuntu 11.04  
**数据来源**：LMAX Disruptor官方文档（three-stage pipeline测试）

**对比数据**：
- **ArrayBlockingQueue Mean Latency**: 32,757ns
- **Disruptor Mean Latency**: 52ns
- **改进**：约**3个数量级**（约630倍）

**关键洞察**：
- Disruptor的平均延迟比ArrayBlockingQueue低**3个数量级**（52ns vs 32,757ns）
- 99%延迟比ArrayBlockingQueue低**4个数量级**（128ns vs 2,097,152ns）
- 这是通过**消除写竞争、最小化读竞争、优化缓存友好性**实现的
- Disruptor延迟（50-200ns）接近硬件基准（L3 cache: 10-20ns, 主内存: 60-80ns）

---

## 表3：硬件延迟 vs 函数调用开销 vs LMAX软件延迟对比

综合对比硬件延迟、函数调用开销和LMAX软件延迟：

| 操作 | 延迟 | 相对于L1 Cache | 相对于LMAX Mean |
|------|------|---------------|----------------|
| **L1 cache hit** | **0.5ns** | 基准（1x） | **1%** |
| **L2 cache hit** | **3-5ns** | 6-10x | **6-10%** |
| **L3 cache hit** | **10-20ns** | 20-40x | **19-38%** |
| **主内存（DDR4）** | **60-80ns** | 120-160x | **115-154%** |
| **虚函数调用** | ~14ns | 28x | **27%** |
| **std::function调用** | ~20-33ns | 40-66x | **38-63%** |
| **LMAX Mean Latency** | **52ns** | 104x | 基准（100%） |
| **当前方案总开销** | ~54-80ns | 108-160x | **104-154%** |
| **方案1总开销** | ~34-47ns | 68-94x | **65-90%** |

**关键洞察**：
1. **硬件延迟层次**：L1 (0.5ns) < L2 (3-5ns) < L3 (10-20ns) < 主内存 (60-80ns)
2. **函数调用开销**：虚函数（14ns）≈ L3 cache（10-20ns），std::function（20-33ns）≈ 主内存（60-80ns）的一半
3. **LMAX软件延迟**：52ns介于L3 cache和主内存之间，说明Disruptor设计非常高效
4. **优化空间**：方案1（34-47ns）更接近L3 cache延迟，当前方案（54-80ns）接近主内存延迟

---

## 函数调用开销参考（3GHz CPU）

| 调用类型 | 时钟周期 | 纳秒 | 说明 |
|---------|---------|------|------|
| **直接函数调用** | ~18周期 | **~6ns** | 非虚函数，编译器可内联 |
| **虚函数调用** | ~42周期 | **~14ns** | 需要查虚函数表（vtable） |
| **std::function调用** | ~60-100周期 | **~20-33ns** | 类型擦除 + 间接调用 |

**数据来源**：基于x86-64架构的典型测量值（可能因CPU架构、编译器优化而异）

---

## 表4：存储和网络延迟补充数据

### 存储设备延迟

| 设备类型 | 操作类型 | 典型延迟 | 3GHz时钟周期 | 说明 |
|---------|---------|---------|-------------|------|
| **HDD（7200 RPM）** | 随机读取 | **8-12ms** | 24M-36M周期 | 寻道时间 + 旋转延迟（4.17ms） |
| **HDD（15000 RPM）** | 随机读取 | **~2ms** | ~6M周期 | 企业级硬盘，平均延迟 |
| **SATA SSD** | 随机读取 | **50-100μs** | 150K-300K周期 | 顺序读取: ~500μs |
| **NVMe SSD** | 随机读取 | **20-100μs** | 60K-300K周期 | 顺序读取: ~100μs |
| **NVMe SSD（高端）** | 随机读取 | **~20μs** | ~60K周期 | PCIe 4.0, 低延迟型号 |

**关键洞察**：
- HDD延迟比SSD高**100-1000倍**（毫秒级 vs 微秒级）
- NVMe SSD比SATA SSD快**2-5倍**
- 存储延迟比内存访问慢**1000-10000倍**（微秒级 vs 纳秒级）
- 随机访问延迟远高于顺序访问（10-100倍差异）

### 网络延迟

| 网络类型 | 场景 | 典型延迟 | 3GHz时钟周期 | 说明 |
|---------|------|---------|-------------|------|
| **Cilium eBPF转发** | 同节点 | **5-50μs** | 15K-150K周期 | 绕过内核，用户空间处理 |
| **Cilium eBPF转发** | 跨AZ | **~0.55ms RTT** | ~1.65M周期 | 往返时间（Round Trip Time） |
| **云商同机房网络** | 同数据中心 | **10-100μs** | 30K-300K周期 | 单跳延迟，低负载 |
| **云商同机房网络** | 跨可用区（AZ） | **0.1-1ms** | 300K-3M周期 | 多跳延迟，可能经过多个交换机 |
| **RDMA（InfiniBand）** | 同机房 | **1-10μs** | 3K-30K周期 | 绕过内核，直接内存访问 |
| **RDMA（RoCE）** | 同机房 | **2-15μs** | 6K-45K周期 | RDMA over Converged Ethernet |

**关键洞察**：
- RDMA延迟（1-10μs）接近**主内存访问延迟**（60-80ns）的**100-1000倍**
- eBPF转发延迟（5-50μs）比传统内核路径快**2-10倍**
- 同机房网络延迟（10-100μs）比跨地域网络（毫秒级）快**10-100倍**
- RDMA是云商提供的**最低延迟网络方案**，适用于HPC、数据库、分布式存储

## 延迟层次总结（从低到高）

### 完整延迟层次（3GHz CPU）

以下是所有延迟数据从低到高的完整排序，便于理解不同操作的相对性能：

| 操作/组件 | 典型范围 | 周期数（最小值） | 备注 |
|---------|---------|-----------------|------|
| **━━━ 纳秒级（ns）━━━** | | | |
| **L1 cache hit** | 0.5-1ns | 1-2 | ⭐ 最快 |
| **L2 cache hit** | 3-5ns | 9-15 | |
| **直接函数调用** | ~6ns | ~18 | |
| **L3 cache hit** | 10-20ns | 30-60 | |
| **虚函数调用** | ~14ns | ~42 | |
| **StoreLoad barrier** | 15-25ns | 45-75 | |
| **Lock-free CAS** | 15-30ns | 45-90 | |
| **std::function调用** | 20-33ns | 60-100 | |
| **Mutex lock/unlock** | 25-50ns | 75-150 | |
| **Disruptor** | 50-200ns | 150-600 | |
| **LMAX Mean Latency** | 52ns | ~156 | 基准值 |
| **DDR4主内存** | 60-80ns | 180-240 | |
| **━━━ 微秒级（μs）━━━** | | | |
| **Aeron IPC** | 0.3-1μs | 900-3000 | |
| **localhost UDP** | 0.5-2μs | 1500-6000 | |
| **RDMA (InfiniBand)** | 1-10μs | 3K-30K | ⭐ 最低网络延迟 |
| **Thread context switch** | 1-5μs | 3K-15K | |
| **RDMA (RoCE)** | 2-15μs | 6K-45K | |
| **Cilium eBPF转发** | 5-50μs | 15K-150K | |
| **localhost TCP** | 5-10μs | 15K-30K | |
| **云商同机房网络** | 10-100μs | 30K-300K | |
| **NVMe SSD随机读取** | 20-100μs | 60K-300K | |
| **SATA SSD随机读取** | 50-100μs | 150K-300K | |
| **━━━ 毫秒级（ms）━━━** | | | |
| **跨AZ网络** | 0.1-1ms | 300K-3M | AZ=可用区（Availability Zone） |
| **Cilium eBPF跨AZ RTT** | ~0.55ms | ~1.65M | 跨可用区往返延迟 |
| **HDD (15000 RPM)** | ~2ms | ~6M | |
| **HDD (7200 RPM)** | 8-12ms | 24M-36M | ⚠️ 最慢 |

> **AZ = 可用区（Availability Zone）**：云服务商在同一地域内的独立数据中心。跨AZ延迟高于同AZ内。
---

## 参考资料

1. **LMAX Disruptor官方文档**：https://lmax-exchange.github.io/disruptor/
2. **Martin Thompson - Mechanical Sympathy**：关于硬件和软件性能的演讲和博客
3. **disruptor-cpp文档**：`third_party/disruptor-cpp/docs/ARCHITECTURE.md`
4. **LMAX Disruptor论文**：Disruptor-1.0.pdf

---

**版本**：1.0  
**最后更新**：2025-12-28  
**维护者**：Exchange-CPP Team

